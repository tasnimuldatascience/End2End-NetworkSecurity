
# End-to-End Network Security with Machine Learning

## Overview
This project implements an **end-to-end machine learning pipeline** to enhance **network security**. It utilizes a scalable and automated workflow to ingest, process, and analyze network data for identifying and mitigating potential threats. The project is designed with modular components for **data ingestion**, **validation**, **transformation**, **model training**, and **deployment**.

Key features include:
- Automated ETL (Extract, Transform, Load) pipeline.
- Machine learning-based threat detection and classification.
- Robust handling of imbalanced datasets using **SMOTETomek**.
- Experiment tracking with **MLflow**.
- Deployment-ready architecture with **Docker** and **AWS** integration.

---

## Project Structure
```plaintext
.
├── .github/workflows       # GitHub Actions for CI/CD
├── Artifacts               # Stores preprocessed data and trained models
├── data_schema             # Data schema definitions for validation
├── final_model             # Final trained model artifacts
├── logs                    # Logs for debugging and monitoring
├── Network_Data            # Raw and processed network data
├── networksecurity         # Core package for network security implementation
├── prediction_output       # Predictions generated by the deployed model
├── templates               # Templates for the web application
├── valid_data              # Validated input data
├── venv                    # Virtual environment for Python dependencies
├── .env                    # Environment variables
├── .gitignore              # Git ignore file
├── app.py                  # Flask application for serving the model
├── Dockerfile              # Docker configuration for containerization
├── main.py                 # Entry point for pipeline execution
├── push_data.py            # Script for pushing data to remote storage
├── README.md               # Project documentation
├── requirements.txt        # Python dependencies
├── setup.py                # Package configuration
└── test_mongodb.py         # MongoDB integration test
```

---

## Features
1. **ETL Pipeline**
   - Configurable pipeline for extracting raw network data.
   - Performs data validation and schema enforcement.

2. **Data Transformation**
   - Handles missing values with **KNN Imputer** and **Simple Imputer**.
   - Scales features using **RobustScaler**.
   - Applies **SMOTETomek** for addressing class imbalance.
   - Produces numpy arrays for model training and evaluation.

3. **Model Training and Experimentation**
   - Training pipeline supports hyperparameter tuning.
   - Tracks experiments using **MLflow** for reproducibility.
   - Saves the best-performing model for deployment.

4. **Deployment**
   - Builds a Docker container for the application.
   - Integrates GitHub Actions for CI/CD.
   - Deploys the application to **AWS EC2** and pushes artifacts to **AWS S3**.

5. **Prediction and Monitoring**
   - Batch prediction implementation for bulk data processing.
   - Logs predictions and updates to the monitoring system.

---

## Workflow

### Data Transformation Pipeline
- **Input**: Raw network data (CSV files).
- **Processing Steps**:
  1. Data validation (schema check).
  2. Imputation of missing values.
  3. Feature scaling and balancing.
  4. Transformation into training and testing numpy arrays.
  5. Artifacts stored as `.npy` and preprocessing pipeline saved as `preprocessing.pkl`.

- **Output**: Transformed data ready for model training.

### Model Training
- Initializes pipeline with hyperparameter tuning.
- Tracks model metrics using **MLflow**.
- Pushes final model artifacts to **AWS S3**.

### Deployment
- Builds a Docker container for hosting the Flask app.
- Deploys containerized app to **AWS EC2** with CI/CD automation.
